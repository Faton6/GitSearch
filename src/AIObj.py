# Standart libs import
import time
from abc import ABC
import json
from typing import Optional, Dict, List, Any
import re
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è AI —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None

from src import constants
from src.logger import logger
from src.exceptions import (
    LLMAPIError, LLMProviderUnavailableError, RateLimitError, AIAnalysisError
)


class AIObj(ABC):
    base_prompt_text = 'None'
    
    def __init__(self, secrets: dict, stats_data: dict, leak_info: dict, company_info: Optional[Dict[str, Any]] = None):
        if TIKTOKEN_AVAILABLE and tiktoken:
            try:
                self.tokenizer = tiktoken.get_encoding("cl100k_base")
            except Exception as e:
                logger.warning(f"tiktoken initialization failed: {e}, using simple tokenizer")
                self.tokenizer = None
        else:
            logger.debug("tiktoken not available, using simple tokenizer")
            self.tokenizer = None
        
        self.ai_requested = False
        self.ai_analysis_completed = False
        
        self.ai_result = -1
        
        self.ai_analysis = None
        self.ai_report = None
        
        self.company_info = company_info or {}
        
        self._llm_manager = None
        
        self._prepare_analysis_data(secrets, stats_data, leak_info)
    
    def _prepare_analysis_data(self, secrets: dict, stats_data: dict, leak_info: dict):
        secrets_str = json.dumps(secrets) if secrets else "-"
        
        if self.tokenizer:
            token_limit = constants.AI_CONFIG.get('token_limit', 4000) - 1000
            if len(self.tokenizer.encode(secrets_str)) > token_limit:
                secrets_str = self.tokenizer.decode(self.tokenizer.encode(secrets_str)[:token_limit])
                secrets_str += '...Cutted, token limit reached.'
        else:
            if len(secrets_str) > 10000:
                secrets_str = secrets_str[:10000] + '...Cutted, char limit reached.'
        
        if secrets:
            raw_report_str = "\n".join(str(item) for item in secrets)
        else:
            raw_report_str = "-"
        
        size_value = self.safe_val(stats_data.get("size"))
        forks_value = self.safe_val(stats_data.get("forks_count"))
        stargazers_value = self.safe_val(stats_data.get("stargazers_count"))
        description_value = self.safe_val(stats_data.get("description"))
        
        contributers_list = leak_info.get("contributers") or []
        commiters_list = leak_info.get("commiters") or []
        
        self.repo_name = self.safe_val(leak_info.get("repo_name"))
        self.author = self.safe_val(leak_info.get("author"))
        self.dork = self.safe_val(leak_info.get("dork"))
        self.created_at = self.safe_val(leak_info.get("created_at"))
        self.updated_at = self.safe_val(leak_info.get("updated_at"))
        
        self.processed_data = {
            "secrets": secrets,
            "secrets_str": secrets_str,
            "raw_report_str": raw_report_str,
            "stats": {
                "size": size_value,
                "forks": forks_value,
                "stargazers": stargazers_value,
                "description": description_value
            },
            "contributors": contributers_list,
            "commiters": commiters_list
        }
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.base_prompt_text = (
            "### Data:\n"
            f"Repository name: {self.repo_name}\n"
            f"Author: {self.author}\n"
            f"Last updated at: {self.updated_at}\n"
            f"Repository created at: {self.created_at}\n"
            f"Stats of repo -> Size: {size_value}, Forks: {forks_value}, Stargazers: {stargazers_value}\n"
            f"Description of repo: {description_value}\n\n"
            f"Contributers:\n{contributers_list}\n\n"
            f"Commiters:\n{commiters_list}\n\n"
            f"Company related dork: {self.dork}\n\n"
            f"Raw_report (may cut off):\n{raw_report_str.replace('\t', '').replace('\n', '')}...\n\n"
        )
                      
    def safe_val(self, val):
        if val is None or val == "":
            return "-"
        return str(val)

    @property
    def llm_manager(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –º–µ–Ω–µ–¥–∂–µ—Ä–∞"""
        if self._llm_manager is None:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä
            global llm_manager
            self._llm_manager = llm_manager
        return self._llm_manager

    def analyze_leak_comprehensive(self):
        """–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ç–µ—á–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""

        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –∞–Ω–∞–ª–∏–∑–∞
        if not self.llm_manager.has_available_providers():
            return None
        
        if not self.llm_manager.providers:
            logger.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤")
            return None

        system_prompt = (
            "You are a security analyst. "
            "Analyse provided repository information and return JSON with the "
            "following structure: {\n"
            "  'company_relevance': { 'is_related': bool, 'confidence': float },\n"
            "  'severity_assessment': { 'level': str, 'score': float },\n"
            "  'classification': { 'true_positive_probability': float },\n"
            "  'summary': str,\n"
            "  'recommendations': str\n"
            "}."
        )

        user_prompt = self.base_prompt_text
        if self.company_info:
            try:
                user_prompt += f"Company info: {json.dumps(self.company_info)}\n"
            except Exception:
                user_prompt += f"Company info: {self.company_info}\n"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        response = self.llm_manager.make_request(
            messages,
            max_tokens=1024,
            temperature=constants.AI_CONFIG.get("temperature", 0.1),
        )

        if not response:
            return None

        try:
            content = response["choices"][0]["message"]["content"].strip()
        except Exception as e:  # pragma: no cover - defensive
            logger.error(f"Error in respone LLM: {e}")
            return None

        analysis_data = None
        try:
            analysis_data = json.loads(content)
        except json.JSONDecodeError:
            match = re.search(r"\{.*\}", content, re.DOTALL)
            if match:
                try:
                    analysis_data = json.loads(match.group(0))
                except Exception:
                    analysis_data = None

        if analysis_data:
            self.ai_analysis = analysis_data
            self.ai_result = 1 if analysis_data.get("company_relevance", {}).get("is_related") else 0
            self.ai_analysis_completed = True
            self.ai_requested = True
            return analysis_data

        if content in {"0", "1"}:
            self.ai_result = int(content)
        else:
            self.ai_result = -1

        self.ai_requested = True
        return None
    
    def safe_generate(
        self,
        prompt: str,
        ctx_size: int = 8192,
        max_new_tokens: int = 1024,
        safety_margin: int = 256
    ) -> tuple[str, int]:
        
        prompt_tokens = self.tokenizer.encode(prompt)
        
        max_prompt_tokens = ctx_size - max_new_tokens - safety_margin
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(prompt_tokens)}")
        
        if len(prompt_tokens) > max_prompt_tokens:
            prompt_tokens = prompt_tokens[:max_prompt_tokens]
            logger.info(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü—Ä–æ–º–ø—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ {max_prompt_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        
        max_tokens = ctx_size-len(prompt_tokens)-5
        prompt = self.tokenizer.decode(prompt_tokens)

        if max_tokens < 0:
            logger.error(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, –ø—Ä–æ–º–ø—Ç –Ω–µ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è. –ü—Ä–æ–º–ø—Ç: {prompt}")
            return ""
        else:
            return prompt, max_tokens

    def lm_studio_request(self, prompt: str, client: str, max_tokens: int, temperature: float, model: str):
        system_prompt = (            
                "### Instruction:\n"
                "You are a data leak detection expert. Analyze the data and respond ONLY with '1' (leak found) or '0' (no leak).\n"
                "Strict response rules:\n"
                "- Output must be single character: 1 or 0\n"
                
                "### Examples:\n"
                "Good: 1\n"
                "Good: 0\n"
                "Bad: I think it's 1 because...\n"
                
                "### Assessment Criteria:\n"
                "Return 1 if the leak may be related to the company\n"
                "Often in leaks very low stars quantity and/or russian related description/authors names/domains (ru)"
                
        )
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "GitSearch_chech",
                    "description": "Function to analyze gitsearch incidents",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "name": {
                                "type": "string",
                                "description": "The incident classification"
                            }
                        },
                        "required": ["data"]
                    }
                }
            }
        ]
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
                #tools=tools,
                stop=["</answer>", "<|im_end|>"]
            )
        except Exception as ex:
            logger.error(f'Api request error: {ex}')
            return None

        return response
    
    def ai_request(self):
        """–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        if self.ai_requested:
            return
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if constants.AI_ANALYSIS_ENABLED and self.llm_manager.providers:
            try:
                analysis = self.analyze_leak_comprehensive()
                if analysis:
                    self.ai_requested = True
                    return
            except Exception as e:
                logger.warning(f"–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥: {str(e)}")
        
        # Fallback –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –º–µ—Ç–æ–¥—É
        result_promt, max_tokens = self.safe_generate(prompt=self.base_prompt_text,
                                                      ctx_size=constants.AI_CONFIG['token_limit'])
        if not result_promt:
            self.ai_result = -1
            self.ai_requested = True
            return

        try:
            client = OpenAI(base_url=constants.AI_CONFIG['url'], api_key=constants.AI_CONFIG['api_key'])
        except Exception as ex:
            logger.error(f'Error in connection to AI API: {ex}')
            self.ai_requested = True
            return
        
        try:
            ai_response = self.lm_studio_request(prompt=result_promt, 
                                                client=client,
                                                max_tokens=max_tokens,
                                                temperature=constants.AI_CONFIG['temperature'],
                                                model=constants.AI_CONFIG['model'])
            if ai_response and ai_response.choices:
                self.ai_analysis = ai_response.choices[0].message.content.strip()
                if self.ai_analysis == '0':
                    self.ai_result = 0
                elif self.ai_analysis == '1':
                    self.ai_result = 1
                else:
                    logger.warning(f"AI returned unexpected output: {self.ai_analysis}")
                    self.ai_result = -1
            else:
                logger.warning("AI response was empty or malformed.")
                self.ai_result = -1
            self.ai_requested = True
        except Exception as ex:
            logger.error(f'Error in AI API request: {ex}')
            self.ai_requested = True
        
    def set_company_info(self, company_info: Dict[str, Any]):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        self.company_info = company_info
        
    def get_comprehensive_analysis(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —É—Ç–µ—á–∫–∏"""
        if not self.ai_analysis_completed:
            return self.analyze_leak_comprehensive()
        return self.ai_analysis
    
    def is_company_related(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–∏ —É—Ç–µ—á–∫–∏ —Å –∫–æ–º–ø–∞–Ω–∏–µ–π"""
        if self.ai_analysis:
            return self.ai_analysis.get('company_relevance', {}).get('is_related', False)
        return False
    
    def get_severity_level(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ —É—Ç–µ—á–∫–∏"""
        if self.ai_analysis:
            return self.ai_analysis.get('severity_assessment', {}).get('level', 'unknown')
        return 'unknown'
    
    def get_true_positive_probability(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏—Å—Ç–∏–Ω–Ω–æ–π —É—Ç–µ—á–∫–∏"""
        if self.ai_analysis:
            return self.ai_analysis.get('classification', {}).get('true_positive_probability', 0.0)
        return 0.0
    
    def get_recommendations(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É—Ç–µ—á–∫–µ"""
        if self.ai_analysis:
            return self.ai_analysis.get('recommendations', {})
        return {}
    
    def get_analysis_summary(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
        if self.ai_analysis:
            return self.ai_analysis.get('summary', '–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω')
        return '–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω'


class LLMProviderManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏.
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - –ü—É–ª HTTP-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å keep-alive
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff
    - –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
    """
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
    RETRY_CONFIG = {
        'total': 3,
        'backoff_factor': 0.5,
        'status_forcelist': [429, 500, 502, 503, 504],
        'allowed_methods': ['POST', 'GET']
    }
    
    def __init__(self):
        self.providers = {}
        self.usage_stats = {}
        self._providers_available = None  # –ö–µ—à –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        self._request_counter = 0  # –°—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        self._last_check_time = 0  # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é —Å –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ retry-–ª–æ–≥–∏–∫–æ–π
        self._session = self._create_session()
        
        self._load_providers()
    
    def _create_session(self) -> requests.Session:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏.
        
        Returns:
            requests.Session —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
        """
        session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º retry-—Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        retry_strategy = Retry(
            total=self.RETRY_CONFIG['total'],
            backoff_factor=self.RETRY_CONFIG['backoff_factor'],
            status_forcelist=self.RETRY_CONFIG['status_forcelist'],
            allowed_methods=self.RETRY_CONFIG['allowed_methods'],
            raise_on_status=False  # –ù–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∞–º–∏
        )
        
        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä —Å –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,  # –ú–∞–∫—Å–∏–º—É–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ –ø—É–ª–µ
            pool_maxsize=10,      # –ú–∞–∫—Å–∏–º—É–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ keep-alive
            pool_block=False      # –ù–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—É–ª–∞
        )
        
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        logger.info("üîå HTTP connection pool initialized with retry strategy")
        return session
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""
        if self._session:
            self._session.close()
            logger.info("üîå HTTP connection pool closed")
    
    def _load_providers(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ constants (—É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ .env)
        env_vars = constants.env_variables
        
        for provider_config in constants.LLM_PROVIDERS:
            api_key = env_vars.get(provider_config["api_key_env"])
            if api_key and api_key.strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–ª—é—á –Ω–µ –ø—É—Å—Ç–æ–π
                self.providers[provider_config["name"]] = {
                    **provider_config,
                    "api_key": api_key,
                    "requests_count": 0,
                    "last_request_time": 0,
                    "error_count": 0
                }
                logger.info(f"Uploaded LLM provider: {provider_config['name']}")
            else:
                logger.warning(f"API key for {provider_config['name']} not found or empty")
    
    def _check_providers_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        current_time = time.time()
        
        for name, provider in self.providers.items():
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
            if provider["error_count"] < 3:
                if provider.get("requests_count", 0) >= provider.get("daily_limit", float("inf")) - 50:
                    continue
                if current_time - provider["last_request_time"] > 60 / provider["rpm"]:
                    return True
        
        return False
    
    def get_available_provider(self) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        current_time = time.time()
        
        for name, provider in self.providers.items():
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤
            if provider["error_count"] < 3:
                if provider.get("requests_count", 0) >= provider.get("daily_limit", float("inf")) - 50:
                    continue
                if current_time - provider["last_request_time"] > 60 / provider["rpm"]:
                    return provider
        
        return None
    
    def has_available_providers(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        check_interval = constants.AI_PROVIDER_CHECK_INTERVAL
        
        # –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª 0, –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º
        if check_interval == 0:
            available = self._check_providers_available()
            self._providers_available = available
            return available
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        self._request_counter += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –∫–µ—à
        if self._providers_available is None or self._request_counter >= check_interval:
            available = self._check_providers_available()
            self._providers_available = available
            self._request_counter = 0
            self._last_check_time = time.time()
            
            if not available:
                logger.warning(f"ZERO Available LLM providers. Next check after {check_interval} requests")
            else:
                logger.info(f"LLM providers are available. Next check after {check_interval} requests")
        
        return self._providers_available
    
    def make_request(self, messages: List[Dict[str, str]], 
                    max_tokens: int = 2000, 
                    temperature: float = 0.1) -> Optional[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ –¥–æ—Å—Ç—É–ø–Ω–æ–º—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É"""
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        if not self.has_available_providers():
            return None
        
        provider = self.get_available_provider()
        if not provider:
            logger.error("ZERO Available LLM providers")
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–µ—à, —Ç.–∫. –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
            self._providers_available = False
            return None
        
        headers = {
            "Authorization": f"Bearer {provider['api_key']}",
            "Content-Type": "application/json"
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è OpenRouter
        if provider["name"] == "openrouter":
            headers["HTTP-Referer"] = "https://github.com/gitsearch"
            headers["X-Title"] = "GitSearch AI Analyzer"
        
        payload = {
            "model": provider["model"],
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é —Å –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏
            response = self._session.post(
                f"{provider['base_url']}/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                provider["requests_count"] += 1
                provider["last_request_time"] = time.time()
                provider["error_count"] = 0
                logger.debug(f"‚úÖ LLM request successful via {provider['name']}")
                return response.json()
            elif response.status_code == 429:
                logger.warning(f"‚ö†Ô∏è Provider {provider['name']} rate limited (429)")
                provider["error_count"] += 1
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                provider["last_request_time"] = time.time() + 60  # –ë–ª–æ–∫–∏—Ä—É–µ–º –Ω–∞ –º–∏–Ω—É—Ç—É
                return None
            else:
                logger.error(f"‚ùå Provider {provider['name']} returned status: {response.status_code}: {response.text[:200]}")
                provider["error_count"] += 1
                return None
        
        except requests.exceptions.Timeout:
            logger.error(f"‚è±Ô∏è Timeout for provider {provider['name']}")
            provider["error_count"] += 1
            return None
        except requests.exceptions.ConnectionError as e:
            logger.error(f"üîå Connection error for provider {provider['name']}: {str(e)}")
            provider["error_count"] += 1
            return None
        except Exception as e:
            logger.error(f"üí• Unexpected error for provider {provider['name']}: {str(e)}")
            provider["error_count"] += 1
            return None


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
llm_manager = LLMProviderManager()
